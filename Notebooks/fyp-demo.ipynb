{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# GROUP MEMBERS:\n# HAASHA BIN ATIF\n# MUHAMAMAD AHMED\n# MUHAMMAD ABDULLAH\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport torch.nn as nn\nimport torchvision\nimport cv2 as cv\nimport PIL\nfrom tqdm.notebook import tqdm\nfrom time import perf_counter\nimport subprocess","execution_count":65,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"thread_identification_MODEL_PATH='../input/additional/2-Way-BestModel.pth'\n#thread_classification_MODEL_PATH='../input/additional/3-Way-NonWeighted-BestModel.pth'\nthread_classification_MODEL_PATH='../input/additional/3-Way-Weighted-BestModel.pth'\n#thread_identification_MODEL_PATH=\"../input/additional/BestModel-4-Fold-Binary.pth\"\n#thread_identification_MODEL_PATH='../input/additional/Model.pth'\n#thread_identification_MODEL_PATH='../input/additional/Model-4-.pth'\n\n#thread_classification_MODEL_PATH=\"../input/additional/BestModel-3-Way.pth\"\n\n#four_way_dict='../input/additional/4-Way-NonWeighted-BestModel.pth'\nfour_way_dict='../input/additional/4-Way-Weighted-BestModel.pth'\n","execution_count":66,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_length(filename):\n    result = subprocess.run([\"ffprobe\", \"-v\", \"error\", \"-show_entries\",\n                             \"format=duration\", \"-of\",\n                             \"default=noprint_wrappers=1:nokey=1\", filename],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT)\n    return float(result.stdout)","execution_count":67,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getFrames(FilePath,n_frame=10):\n    images=[]\n    VideoCap = cv.VideoCapture(FilePath)\n    \n    i=0\n    while(True):\n        hasFrames,image = VideoCap.read()\n        \n        if i%n_frame==0 and hasFrames:\n            images.append(image)\n            i+=1\n            continue\n        if not hasFrames:\n            break\n        i+=1\n    return images\ndef calculate_first_mask(frames,skip=10):\n    frame1=frames[0]\n    gray1 = cv.cvtColor(frame1, cv.COLOR_BGR2GRAY)\n    gray1 = cv.GaussianBlur(gray1, (3, 3), 0)\n    length,width,color=frames[0].shape\n    mask=np.zeros((length,width),dtype=np.uint8)\n    dimension=frame1.shape[0]*frame1.shape[1]\n    for i in range(1,skip):\n        frame2=frames[i]\n        gray2 = cv.cvtColor(frame2, cv.COLOR_BGR2GRAY)\n        gray2 = cv.GaussianBlur(gray2, (3, 3), 0)\n        deltaframe=cv.absdiff(gray1,gray2)\n        threshold = cv.threshold(deltaframe, 20, i, cv.THRESH_BINARY)[1]\n        threshold = cv.dilate(threshold,None)\n        zero_frame=len(gray2[gray2==0])\n        if zero_frame<dimension:\n            mask=np.maximum(mask,threshold)\n    return mask\ndef apply_mask(frames,skip=10):\n    output=[]\n    mask=calculate_first_mask(frames,skip)\n    firstmask=np.zeros(frames[0].shape,dtype=np.uint8)\n    firstmask[mask>0]=1\n    tempo=frames[1]*firstmask\n    dimension=frames[0].shape[0]*frames[0].shape[1]\n    for i in range(len(frames)):\n        temp=np.zeros((frames[0].shape),np.uint8)\n        temp[mask>0]=1\n        output.append(frames[i]*temp)\n        gray2 = cv.cvtColor(frames[i], cv.COLOR_BGR2GRAY)\n        gray2 = cv.GaussianBlur(gray2, (3, 3), 0)\n        zero_frame=len(np.where(gray2==0))\n        mask[mask>0]-=1\n        if i>0:\n            deltaframe=cv.absdiff(gray1,gray2)\n            threshold = cv.threshold(deltaframe, 20, skip, cv.THRESH_BINARY)[1]\n            threshold = cv.dilate(threshold,None,iterations=20)\n            zero_frame=len(gray2[gray2==0])\n            if zero_frame<dimension:\n                mask=np.maximum(mask,threshold)\n        gray1=gray2\n    return output","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class vggCNNEncoder(nn.Module):\n    def __init__(self, fc_hidden1=512, fc_hidden2=512, drop_p=0.3, CNN_embed_dim=300,h_RNN_layers=3, h_RNN=256, h_FC_dim=128,  num_classes=3):\n        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n        super(vggCNNEncoder, self).__init__()\n\n        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\n        self.drop_p = drop_p\n        self.RNN_input_size = CNN_embed_dim\n        self.h_RNN_layers = h_RNN_layers   # RNN hidden layers\n        self.h_RNN = h_RNN                 # RNN hidden nodes\n        self.h_FC_dim = h_FC_dim\n        self.drop_p = drop_p\n        self.num_classes = num_classes\n        \n        vgg16 = torchvision.models.vgg16(pretrained=False)\n        modules = list(vgg16.children())[:-1]      # delete the last fc layer.\n        self.vgg16 = nn.Sequential(*modules)\n        self.fc1 = nn.Linear(vgg16.classifier[0].in_features, fc_hidden1)\n        self.bn1 = nn.BatchNorm1d(fc_hidden1, momentum=0.01)\n        self.fc2 = nn.Linear(fc_hidden1, fc_hidden2)\n        self.bn2 = nn.BatchNorm1d(fc_hidden2, momentum=0.01)\n        self.fc3 = nn.Linear(fc_hidden2, CNN_embed_dim)\n\n        self.LSTM = nn.LSTM(\n                        input_size=self.RNN_input_size,\n                        hidden_size=self.h_RNN,        \n                        num_layers=h_RNN_layers,       \n                        batch_first=True)       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size))\n        self.fc4 = nn.Linear(self.h_RNN, self.h_FC_dim)\n        self.fc5 = nn.Linear(self.h_FC_dim, self.num_classes)\n\n    def forward(self, x_3d):\n        cnn_embed_seq = []\n        for t in range(x_3d.size(1)):\n            with torch.no_grad():\n                x = self.vgg16(x_3d[:, t, :, :, :])  # ResNet\n                x = x.view(x.size(0), -1)             # flatten output of conv\n\n            # FC layers\n            x = self.bn1(self.fc1(x))\n            x = nn.functional.relu(x)\n            x = nn.functional.dropout(x, p=self.drop_p, training=self.training)\n            x = self.bn2(self.fc2(x))\n            x = nn.functional.relu(x)\n            x = nn.functional.dropout(x, p=self.drop_p, training=self.training)\n            x = self.fc3(x)\n\n            cnn_embed_seq.append(x)\n        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\n\n        self.LSTM.flatten_parameters()\n        RNN_out, (h_n, h_c) = self.LSTM(cnn_embed_seq, None)  \n        x = self.fc4(RNN_out[:, -1, :])   # choose RNN_out at the last time step\n        x = nn.functional.relu(x)\n        x = nn.functional.dropout(x, p=self.drop_p, training=self.training)\n        x = self.fc5(x)\n        return x","execution_count":69,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"identification_Dict = torch.load(thread_identification_MODEL_PATH)\nclassification_Dict = torch.load(thread_classification_MODEL_PATH)\nfour_classes_dict=torch.load(four_way_dict)\nidentification_model = vggCNNEncoder(num_classes=2)\nclassification_model = vggCNNEncoder(num_classes=3)\nfour_class_model=vggCNNEncoder(num_classes=4)\n\nidentification_model.load_state_dict(identification_Dict['Model'])\nclassification_model.load_state_dict(classification_Dict['Model'])\nfour_class_model.load_state_dict(four_classes_dict['Model'])\nidentification_model.cuda()\nclassification_model.cuda()\nfour_class_model.cuda()\n","execution_count":70,"outputs":[{"output_type":"execute_result","execution_count":70,"data":{"text/plain":"vggCNNEncoder(\n  (vgg16): Sequential(\n    (0): Sequential(\n      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): ReLU(inplace=True)\n      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (3): ReLU(inplace=True)\n      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (6): ReLU(inplace=True)\n      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (8): ReLU(inplace=True)\n      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (11): ReLU(inplace=True)\n      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (13): ReLU(inplace=True)\n      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (15): ReLU(inplace=True)\n      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (18): ReLU(inplace=True)\n      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (20): ReLU(inplace=True)\n      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (22): ReLU(inplace=True)\n      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (25): ReLU(inplace=True)\n      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (27): ReLU(inplace=True)\n      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (29): ReLU(inplace=True)\n      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (1): AdaptiveAvgPool2d(output_size=(7, 7))\n  )\n  (fc1): Linear(in_features=25088, out_features=512, bias=True)\n  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n  (fc2): Linear(in_features=512, out_features=512, bias=True)\n  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n  (fc3): Linear(in_features=512, out_features=300, bias=True)\n  (LSTM): LSTM(300, 256, num_layers=3, batch_first=True)\n  (fc4): Linear(in_features=256, out_features=128, bias=True)\n  (fc5): Linear(in_features=128, out_features=4, bias=True)\n)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"Files = [\"../input/ucf-crime-dataset/Arson/Arson008_x264.mp4\",\"../input/ucf-crime-dataset/Explosion/Explosion037_x264.mp4\",\"../input/ucf-crime-dataset/Fighting/Fighting012_x264.mp4\",\"../input/ucf-crime-dataset/Normal_1/Normal_Videos008_x264.mp4\",\"../input/ucf-crime-dataset/Normal_1/Normal_Videos066_x264.mp4\"]\nFILEPATH='../input/ucf-crime-dataset/Normal_1/Normal_Videos008_x264.mp4''\nfiles_for_demo=['../input/ucf-crime-dataset/Fighting/Fighting028_x264.mp4','../input/ucf-crime-dataset/Arson/Arson024_x264.mp4','../input/ucf-crime-dataset/Normal_1/Normal_Videos008_x264.mp4']\n#FILEPATH='../input/ucf-crime-dataset/Arson/Arson014_x264.mp4'\n\n","execution_count":80,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"EOL while scanning string literal (<ipython-input-80-dfdc0ed286ff>, line 2)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-80-dfdc0ed286ff>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    FILEPATH='../input/ucf-crime-dataset/Normal_1/Normal_Videos008_x264.mp4''\u001b[0m\n\u001b[0m                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class VideoLoader(torch.utils.data.Dataset):\n    def __init__(self,FilePath, n_frame =10, sequence_length = 16,transform = None):\n        self.FilePath=FilePath\n        self.transform=transform\n        if transform is None:\n            self.transform = torchvision.transforms.ToTensor()\n        self.n_frame = n_frame\n        self.sequence_length = sequence_length\n        self.LoadVideo()\n        \n    def LoadVideo(self):\n        self.Frames = getFrames(self.FilePath,n_frame=self.n_frame)\n        \n    def __len__(self):\n        return len(self.Frames)-self.sequence_length\n\n    def __getitem__(self,idx):\n        Frames =self.Frames[idx:idx+self.sequence_length]\n        Frames=apply_mask(Frames,10)\n        Frames = [self.transform(PIL.Image.fromarray(Frame)) for Frame in Frames]\n        Frames = torch.stack(Frames)\n        Frames = Frames.reshape(self.sequence_length,3,264,264)\n        return Frames","execution_count":81,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Transform = torchvision.transforms.Compose([torchvision.transforms.Resize((264,264)),torchvision.transforms.ToTensor()])\nSEQUENCE_LENGTH = 18\nTest = VideoLoader(FILEPATH,n_frame=5,sequence_length=SEQUENCE_LENGTH,transform = Transform)\nTestLoader = torch.utils.data.DataLoader(Test,batch_size=1,num_workers=4,shuffle=False)\n","execution_count":82,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SOFTMAX = nn.Softmax(dim=1)","execution_count":83,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Binary Classification\n# OFTMAX = nn.Softmax(dim=1)\n# identification_model.eval()\n# List = []\n# timing=[]\n# Prev=float('inf')\n# VideoCap = cv.VideoCapture(FILEPATH)\n# start=perf_counter()\n# for idx,data in enumerate(tqdm(TestLoader,desc='Testing')):\n#     Images = data.cuda()\n#     Results = identification_model(Images)\n#     T = SOFTMAX(Results.to(\"cpu\"))\n#     List.append(T[0][0])\n#     if T[0][0]>0.48:\n#         timing.append([((idx*5)),(((idx*5+SEQUENCE_LENGTH*5)))])\n# frames_with_threat=[]\n# for i in timing:\n#     temp=np.arange(i[0],i[1])\n#     frames_with_threat.append(temp)\n# frames_with_threat=np.unique(np.array(frames_with_threat))\n# images=[]\n# hasFrames,image = VideoCap.read()\n# video=cv.VideoWriter(FILEPATH[:-3].split('/')[-1]+'avi',cv.VideoWriter_fourcc(*'DIVX'), 30,(image.shape[1],image.shape[0]))\n# #images.append(image)\n# video.write(image)\n# total=1\n# font = cv.FONT_HERSHEY_PLAIN  \n# while hasFrames:\n#     hasFrames,image = VideoCap.read()\n#     if hasFrames:\n#         total+=1\n# #        images.append(image)\n#         if total in frames_with_threat:\n#             cv.putText(image,'THREAT',(5, 15),font,1,(0,0,255),1,cv.LINE_4) \n#             video.write(image)\n#         else:\n#             cv.putText(image,'NON-THREAT',(5, 15),font,1,(255, 255, 255),1,cv.LINE_4) \n#             video.write(image)\n\n# end=perf_counter()\n# print('Time Taken:',end-start,'Video Duration:',total//30)\n# video.release()","execution_count":84,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 3 class classification\n# identification_model.eval()\n# classification_model.eval()\n\n# List = []\n# timing=[]\n# classes={0:'Fighting',1:'Gun Event',2:'Arson_Explosion'}\n# Prev=float('inf')\n# VideoCap = cv.VideoCapture(FILEPATH)\n# start=perf_counter()\n# length=int(VideoCap.get(cv.CAP_PROP_FRAME_COUNT))\n# array=[-1 for i in range(length)]\n# print(len(array))\n# for idx,data in enumerate(tqdm(TestLoader,desc='Testing')):\n#     Images = data.cuda()\n#     Results = identification_model(Images)\n#     T = SOFTMAX(Results.to(\"cpu\"))\n#     List.append(T[0][1])\n#     if T[0][1]>0.6:\n#         timing.append([((idx*5)),(((idx*5+SEQUENCE_LENGTH*5)))])\n#         classify=classification_model(Images)\n#         T=SOFTMAX(classify.to('cpu')).detach().numpy()\n#         index=np.argmax(T[0])\n#         temp=np.arange(timing[-1][0],timing[-1][1])\n# #        print(temp)\n# #        if array[temp[0]]==-1:\n#         for i in range(idx*5,idx*5+SEQUENCE_LENGTH*5):  \n#             if i==len(array):\n#                 break\n#             array[i]=index            \n        \n# images=[]\n# hasFrames,image = VideoCap.read()\n# video=cv.VideoWriter(FILEPATH[:-3].split('/')[-1]+'avi',cv.VideoWriter_fourcc(*'DIVX'), 30,(image.shape[1],image.shape[0]))\n# #images.append(image)\n# video.write(image)\n# total=1\n# font = cv.FONT_HERSHEY_PLAIN  \n# count=0\n# while hasFrames:\n#     hasFrames,image = VideoCap.read()\n#     if hasFrames:\n#         total+=1\n# #        images.append(image)\n#         if array[count]!=-1:\n#             cv.putText(image,'THREAT:'+classes[array[count]],(5, 15),font,1,(0,0,255),1,cv.LINE_4) \n#             video.write(image)\n#         else:\n#             cv.putText(image,'NON-THREAT',(5, 15),font,1,(255, 255, 255),1,cv.LINE_4) \n#             video.write(image)\n#     count+=1\n# end=perf_counter()\n# print('Time Taken:',end-start,'Video Duration:',total//30)\n# video.release()","execution_count":85,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4 class classification\n\n#identification_model.eval()\n#classification_model.eval()\nfour_class_model.eval()\nList = []\ntiming=[]\nclasses={0:'Fighting',1:'Gun Event',2:'Arson_Explosion',3:'Normal'}\nPrev=float('inf')\nVideoCap = cv.VideoCapture(FILEPATH)\nstart=perf_counter()\nlength=int(VideoCap.get(cv.CAP_PROP_FRAME_COUNT))\narray=[-1 for i in range(length)]\nprint(len(array))\nfor idx,data in enumerate(tqdm(TestLoader,desc='Testing')):\n    Images = data.cuda()\n    timing.append([((idx*5)),(((idx*5+SEQUENCE_LENGTH*5)))])\n    classify=four_class_model(Images)\n    T=SOFTMAX(classify.to('cpu')).detach().numpy()\n    index=np.argmax(T[0])\n    temp=np.arange(timing[-1][0],timing[-1][1])\n#        print(temp)\n#        if array[temp[0]]==-1:\n    for i in range(idx*5,idx*5+SEQUENCE_LENGTH*5):  \n        if i==len(array):\n            break\n        array[i]=index            ","execution_count":null,"outputs":[{"output_type":"stream","text":"2423\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Testing', max=467.0, style=ProgressStyle(description_widt…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01904c713c434614bb4e6d72b310286f"}},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"VideoCap = cv.VideoCapture(FILEPATH)\nimages=[]\nhasFrames,image = VideoCap.read()\nvideo=cv.VideoWriter(FILEPATH[:-3].split('/')[-1]+'avi',cv.VideoWriter_fourcc(*'DIVX'), 30,(image.shape[1],image.shape[0]))\n#images.append(image)\nvideo.write(image)\ntotal=1\nfont = cv.FONT_HERSHEY_PLAIN  \ncount=0\nwhile hasFrames:\n    hasFrames,image = VideoCap.read()\n    if hasFrames:\n        total+=1\n#        images.append(image)\n        if array[count]!=3:\n            if array[count]==-1:\n                break\n            print(array[count])\n            cv.putText(image,'THREAT:'+classes[array[count]],(5, 15),font,1,(0,0,255),1,cv.LINE_4) \n            video.write(image)\n        else:\n            cv.putText(image,'NON-THREAT',(5, 15),font,1,(255, 255, 255),1,cv.LINE_4) \n            video.write(image)\n    count+=1\nend=perf_counter()\nprint('Time Taken:',end-start,'Video Duration:',total//30)\nvideo.release()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(frames_with_threat)\n# VideoCap = cv.VideoCapture(FILEPATH)\n# index=0\n# threat_frames=[]\n# while True:\n#     hasFrames,frame=VideoCap.read()\n#     if hasFrames==False:\n#         break\n#     if index in frames_with_threat:\n#         threat_frames.append(frame)\n#     index+=1","execution_count":78,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'frames_with_threat' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-78-5758cf2d0477>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes_with_threat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mVideoCap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFILEPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mthreat_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'frames_with_threat' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(len(threat_frames),type(threat_frames))\n# threat_frames=np.array(threat_frames)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Frames=apply_mask(threat_frames,10)\n# Frames = [Transform(PIL.Image.fromarray(Frames)) for Frames in Frames]\n# Frames = torch.stack(Frames)\n# Frames = Frames.reshape(self.sequence_length,3,264,264)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weight1=1-49/(181)\nweight2=1-44/(181)\nweight3=1-88/(181)\nprint(weight1,weight2,weight3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weight1=1-49/(181+49)\nweight2=1-44/(181+44)\nweight3=1-88/(181+88)\nprint(weight1,weight2,weight3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"nbformat_minor":4}
