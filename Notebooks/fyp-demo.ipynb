{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# GROUP MEMBERS:\n# MUHAMMAD ABDULLAH\n# HAASHA BIN ATIF\n# MUHAMAMAD AHMED\n ","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport os\nimport pickle\nimport torch.nn as nn\nimport torchvision\nimport cv2 as cv\nimport PIL\nfrom tqdm.notebook import tqdm\nfrom time import perf_counter\nimport subprocess","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"thread_identification_MODEL_PATH='../input/additional/2-Way-BestModel.pth'\n\nthread_classification_nonweighted_MODEL_PATH='../input/additional/3-Way-NonWeighted-BestModel.pth'\nthread_classification_weighted_MODEL_PATH='../input/additional/3-Way-Weighted-BestModel.pth'\n\nfour_class_nonweighted_model='../input/additional/4-Way-NonWeighted-BestModel.pth'\nfour_class_weighted_model='../input/additional/4-Way-Weighted-BestModel.pth'\n","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_length(filename):\n    result = subprocess.run([\"ffprobe\", \"-v\", \"error\", \"-show_entries\",\n                             \"format=duration\", \"-of\",\n                             \"default=noprint_wrappers=1:nokey=1\", filename],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT)\n    return float(result.stdout)","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getFrames(FilePath,n_frame=10):\n    images=[]\n    VideoCap = cv.VideoCapture(FilePath)\n    \n    i=0\n    while(True):\n        hasFrames,image = VideoCap.read()\n        \n        if i%n_frame==0 and hasFrames:\n            images.append(image)\n            i+=1\n            continue\n        if not hasFrames:\n            break\n        i+=1\n    return images\ndef calculate_first_mask(frames,skip=10):\n    frame1=frames[0]\n    gray1 = cv.cvtColor(frame1, cv.COLOR_BGR2GRAY)\n    gray1 = cv.GaussianBlur(gray1, (3, 3), 0)\n    length,width,color=frames[0].shape\n    mask=np.zeros((length,width),dtype=np.uint8)\n    dimension=frame1.shape[0]*frame1.shape[1]\n    for i in range(1,skip):\n        frame2=frames[i]\n        gray2 = cv.cvtColor(frame2, cv.COLOR_BGR2GRAY)\n        gray2 = cv.GaussianBlur(gray2, (3, 3), 0)\n        deltaframe=cv.absdiff(gray1,gray2)\n        threshold = cv.threshold(deltaframe, 20, i, cv.THRESH_BINARY)[1]\n        threshold = cv.dilate(threshold,None)\n        zero_frame=len(gray2[gray2==0])\n        if zero_frame<dimension:\n            mask=np.maximum(mask,threshold)\n    return mask\ndef apply_mask(frames,skip=10):\n    output=[]\n    mask=calculate_first_mask(frames,skip)\n    firstmask=np.zeros(frames[0].shape,dtype=np.uint8)\n    firstmask[mask>0]=1\n    tempo=frames[1]*firstmask\n    dimension=frames[0].shape[0]*frames[0].shape[1]\n    for i in range(len(frames)):\n        temp=np.zeros((frames[0].shape),np.uint8)\n        temp[mask>0]=1\n        output.append(frames[i]*temp)\n        gray2 = cv.cvtColor(frames[i], cv.COLOR_BGR2GRAY)\n        gray2 = cv.GaussianBlur(gray2, (3, 3), 0)\n        zero_frame=len(np.where(gray2==0))\n        mask[mask>0]-=1\n        if i>0:\n            deltaframe=cv.absdiff(gray1,gray2)\n            threshold = cv.threshold(deltaframe, 20, skip, cv.THRESH_BINARY)[1]\n            threshold = cv.dilate(threshold,None,iterations=20)\n            zero_frame=len(gray2[gray2==0])\n            if zero_frame<dimension:\n                mask=np.maximum(mask,threshold)\n        gray1=gray2\n    return output","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class vggCNNEncoder(nn.Module):\n    def __init__(self, fc_hidden1=512, fc_hidden2=512, drop_p=0.3, CNN_embed_dim=300,h_RNN_layers=3, h_RNN=256, h_FC_dim=128,  num_classes=3):\n        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n        super(vggCNNEncoder, self).__init__()\n\n        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\n        self.drop_p = drop_p\n        self.RNN_input_size = CNN_embed_dim\n        self.h_RNN_layers = h_RNN_layers   # RNN hidden layers\n        self.h_RNN = h_RNN                 # RNN hidden nodes\n        self.h_FC_dim = h_FC_dim\n        self.drop_p = drop_p\n        self.num_classes = num_classes\n        \n        vgg16 = torchvision.models.vgg16(pretrained=False)\n        modules = list(vgg16.children())[:-1]      # delete the last fc layer.\n        self.vgg16 = nn.Sequential(*modules)\n        self.fc1 = nn.Linear(vgg16.classifier[0].in_features, fc_hidden1)\n        self.bn1 = nn.BatchNorm1d(fc_hidden1, momentum=0.01)\n        self.fc2 = nn.Linear(fc_hidden1, fc_hidden2)\n        self.bn2 = nn.BatchNorm1d(fc_hidden2, momentum=0.01)\n        self.fc3 = nn.Linear(fc_hidden2, CNN_embed_dim)\n\n        self.LSTM = nn.LSTM(\n                        input_size=self.RNN_input_size,\n                        hidden_size=self.h_RNN,        \n                        num_layers=h_RNN_layers,       \n                        batch_first=True)       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size))\n        self.fc4 = nn.Linear(self.h_RNN, self.h_FC_dim)\n        self.fc5 = nn.Linear(self.h_FC_dim, self.num_classes)\n\n    def forward(self, x_3d):\n        cnn_embed_seq = []\n        for t in range(x_3d.size(1)):\n            with torch.no_grad():\n                x = self.vgg16(x_3d[:, t, :, :, :])  # ResNet\n                x = x.view(x.size(0), -1)             # flatten output of conv\n\n            # FC layers\n            x = self.bn1(self.fc1(x))\n            x = nn.functional.relu(x)\n            x = nn.functional.dropout(x, p=self.drop_p, training=self.training)\n            x = self.bn2(self.fc2(x))\n            x = nn.functional.relu(x)\n            x = nn.functional.dropout(x, p=self.drop_p, training=self.training)\n            x = self.fc3(x)\n\n            cnn_embed_seq.append(x)\n        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\n\n        self.LSTM.flatten_parameters()\n        RNN_out, (h_n, h_c) = self.LSTM(cnn_embed_seq, None)  \n        x = self.fc4(RNN_out[:, -1, :])   # choose RNN_out at the last time step\n        x = nn.functional.relu(x)\n        x = nn.functional.dropout(x, p=self.drop_p, training=self.training)\n        x = self.fc5(x)\n        return x","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#weights loading\nidentification_Dict = torch.load(thread_identification_MODEL_PATH)\nclassification_nonweighted_Dict = torch.load(thread_classification_nonweighted_MODEL_PATH)\nclassification_weighted_Dict = torch.load(thread_classification_weighted_MODEL_PATH)\nfour_class_nonweighted_dict=torch.load(four_class_nonweighted_model)\nfour_class_weighted_dict=torch.load(four_class_weighted_model)\n#model making\nidentification_model = vggCNNEncoder(num_classes=2)\nclassification_nonweighted_model = vggCNNEncoder(num_classes=3)\nclassification_weighted_model = vggCNNEncoder(num_classes=3)\nfour_class_nonweighted_model=vggCNNEncoder(num_classes=4)\nfour_class_weighted_model=vggCNNEncoder(num_classes=4)\n#Assigning weights to its model\nidentification_model.load_state_dict(identification_Dict['Model'])\nclassification_nonweighted_model.load_state_dict(classification_nonweighted_Dict['Model'])\nclassification_weighted_model.load_state_dict(classification_weighted_Dict['Model'])\nfour_class_nonweighted_model.load_state_dict(four_class_nonweighted_dict['Model'])\nfour_class_weighted_model.load_state_dict(four_class_weighted_dict['Model'])\n#shifting to gpu\nidentification_model.cuda()\nclassification_nonweighted_model.cuda()\nclassification_weighted_model.cuda()\nfour_class_nonweighted_model.cuda()\nfour_class_weighted_model.cuda()\n","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"vggCNNEncoder(\n  (vgg16): Sequential(\n    (0): Sequential(\n      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): ReLU(inplace=True)\n      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (3): ReLU(inplace=True)\n      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (6): ReLU(inplace=True)\n      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (8): ReLU(inplace=True)\n      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (11): ReLU(inplace=True)\n      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (13): ReLU(inplace=True)\n      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (15): ReLU(inplace=True)\n      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (18): ReLU(inplace=True)\n      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (20): ReLU(inplace=True)\n      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (22): ReLU(inplace=True)\n      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (25): ReLU(inplace=True)\n      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (27): ReLU(inplace=True)\n      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (29): ReLU(inplace=True)\n      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (1): AdaptiveAvgPool2d(output_size=(7, 7))\n  )\n  (fc1): Linear(in_features=25088, out_features=512, bias=True)\n  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n  (fc2): Linear(in_features=512, out_features=512, bias=True)\n  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n  (fc3): Linear(in_features=512, out_features=300, bias=True)\n  (LSTM): LSTM(300, 256, num_layers=3, batch_first=True)\n  (fc4): Linear(in_features=256, out_features=128, bias=True)\n  (fc5): Linear(in_features=128, out_features=4, bias=True)\n)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#FILEPATH='../input/ucf-crime-dataset/Fighting/Fighting028_x264.mp4'\nfiles_for_demo=['../input/ucf-crime-dataset/Fighting/Fighting025_x264.mp4','../input/ucf-crime-dataset/Arson/Arson024_x264.mp4','../input/ucf-crime-dataset/Normal_1/Normal_Videos008_x264.mp4']","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class VideoLoader(torch.utils.data.Dataset):\n    def __init__(self,FilePath, n_frame =10, sequence_length = 16,transform = None):\n        self.FilePath=FilePath\n        self.transform=transform\n        if transform is None:\n            self.transform = torchvision.transforms.ToTensor()\n        self.n_frame = n_frame\n        self.sequence_length = sequence_length\n        self.LoadVideo()\n        \n    def LoadVideo(self):\n        self.Frames = getFrames(self.FilePath,n_frame=self.n_frame)\n        \n    def __len__(self):\n        return len(self.Frames)-self.sequence_length\n\n    def __getitem__(self,idx):\n        Frames =self.Frames[idx:idx+self.sequence_length]\n        Frames=apply_mask(Frames,10)\n        Frames = [self.transform(PIL.Image.fromarray(Frame)) for Frame in Frames]\n        Frames = torch.stack(Frames)\n        Frames = Frames.reshape(self.sequence_length,3,264,264)\n        return Frames","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Transform = torchvision.transforms.Compose([torchvision.transforms.Resize((264,264)),torchvision.transforms.ToTensor()])\nSEQUENCE_LENGTH = 18\n#Test = VideoLoader(files_for_demo,n_frame=5,sequence_length=SEQUENCE_LENGTH,transform = Transform)\n#TestLoader = torch.utils.data.DataLoader(Test,batch_size=1,num_workers=4,shuffle=False)\n","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SOFTMAX = nn.Softmax(dim=1)","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Binary Classification\n# OFTMAX = nn.Softmax(dim=1)\n# identification_model.eval()\n# List = []\n# timing=[]\n# Prev=float('inf')\n# VideoCap = cv.VideoCapture(FILEPATH)\n# start=perf_counter()\n# for idx,data in enumerate(tqdm(TestLoader,desc='Testing')):\n#     Images = data.cuda()\n#     Results = identification_model(Images)\n#     T = SOFTMAX(Results.to(\"cpu\"))\n#     List.append(T[0][0])\n#     if T[0][0]>0.48:\n#         timing.append([((idx*5)),(((idx*5+SEQUENCE_LENGTH*5)))])\n# frames_with_threat=[]\n# for i in timing:\n#     temp=np.arange(i[0],i[1])\n#     frames_with_threat.append(temp)\n# frames_with_threat=np.unique(np.array(frames_with_threat))\n# images=[]\n# hasFrames,image = VideoCap.read()\n# video=cv.VideoWriter(FILEPATH[:-3].split('/')[-1]+'avi',cv.VideoWriter_fourcc(*'DIVX'), 30,(image.shape[1],image.shape[0]))\n# #images.append(image)\n# video.write(image)\n# total=1\n# font = cv.FONT_HERSHEY_PLAIN  \n# while hasFrames:\n#     hasFrames,image = VideoCap.read()\n#     if hasFrames:\n#         total+=1\n# #        images.append(image)\n#         if total in frames_with_threat:\n#             cv.putText(image,'THREAT',(5, 15),font,1,(0,0,255),1,cv.LINE_4) \n#             video.write(image)\n#         else:\n#             cv.putText(image,'NON-THREAT',(5, 15),font,1,(255, 255, 255),1,cv.LINE_4) \n#             video.write(image)\n\n# end=perf_counter()\n# print('Time Taken:',end-start,'Video Duration:',total//30)\n# video.release()","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3 class classification\ndef hierarchical_model(FILEPATH,identification_model,classification_model,model_name):\n    \n    identification_model.eval()\n    classification_model.eval()\n\n    List = []\n    timing=[]\n    classes={0:'Fighting',1:'Gun Event',2:'Arson_Explosion'}\n    Prev=float('inf')\n    VideoCap = cv.VideoCapture(FILEPATH)\n    start=perf_counter()\n    length=int(VideoCap.get(cv.CAP_PROP_FRAME_COUNT))\n    array=[-1 for i in range(length)]\n    print(len(array))\n    for idx,data in enumerate(TestLoader):#tqdm(TestLoader,desc='Testing 3-way-'+model_name)):\n        Images = data.cuda()\n        Results = identification_model(Images)\n        T = SOFTMAX(Results.to(\"cpu\"))\n        List.append(T[0][1])\n        \n        if T[0][1]>0.6:\n            timing.append([((idx*5)),(((idx*5+SEQUENCE_LENGTH*5)))])\n            classify=classification_model(Images)\n            T=SOFTMAX(classify.to('cpu')).detach().numpy()\n            index=np.argmax(T[0])\n            temp=np.arange(timing[-1][0],timing[-1][1])\n    #        print(temp)\n    #        if array[temp[0]]==-1:\n            for i in range(idx*5,idx*5+SEQUENCE_LENGTH*5):  \n                if i==len(array):\n                    break\n                array[i]=index            \n\n    images=[]\n    hasFrames,image = VideoCap.read()\n    video=cv.VideoWriter('3-way-'+model_name+'-'+FILEPATH[:-3].split('/')[-1]+'avi',cv.VideoWriter_fourcc(*'DIVX'), 30,(image.shape[1],image.shape[0]))\n    #images.append(image)\n    video.write(image)\n    total=1\n    font = cv.FONT_HERSHEY_PLAIN  \n    count=0\n    while hasFrames:\n        hasFrames,image = VideoCap.read()\n        if hasFrames:\n            total+=1\n    #        images.append(image)\n            if array[count]!=-1:\n                cv.putText(image,'THREAT:'+classes[array[count]],(5, 15),font,1,(0,0,255),1,cv.LINE_4) \n                video.write(image)\n            else:\n                cv.putText(image,'NON-THREAT',(5, 15),font,1,(255, 255, 255),1,cv.LINE_4) \n                video.write(image)\n        count+=1\n    end=perf_counter()\n    print('Time Taken:',end-start,'Video Duration:',total//30)\n    video.release()","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4 class classification\ndef single_model(FILEPATH,four_class_model,model_name):\n    four_class_model.eval()\n    List = []\n    timing=[]\n    classes={0:'Fighting',1:'Gun Event',2:'Arson_Explosion',3:'Normal'}\n    Prev=float('inf')\n    VideoCap = cv.VideoCapture(FILEPATH)\n    start=perf_counter()\n    length=int(VideoCap.get(cv.CAP_PROP_FRAME_COUNT))\n    array=[-1 for i in range(length)]\n    print(len(array))\n    for idx,data in enumerate(TestLoader):#tqdm(TestLoader,desc='Testing 4-way-'+model_name)):\n        Images = data.cuda()\n        timing.append([((idx*5)),(((idx*5+SEQUENCE_LENGTH*5)))])\n        classify=four_class_model(Images)\n        T=SOFTMAX(classify.to('cpu')).detach().numpy()\n        index=np.argmax(T[0])\n    #    print(T,index)\n        temp=np.arange(timing[-1][0],timing[-1][1])\n    #        print(temp)\n    #        if array[temp[0]]==-1:\n        for i in range(idx*5,idx*5+SEQUENCE_LENGTH*5):  \n            if i>=len(array)-1:\n                break\n            array[i]=index            \n    images=[]\n    hasFrames,image = VideoCap.read()\n    video=cv.VideoWriter('4-way-'+model_name+'-'+FILEPATH[:-3].split('/')[-1]+'avi',cv.VideoWriter_fourcc(*'DIVX'), 30,(image.shape[1],image.shape[0]))\n    #images.append(image)\n    video.write(image)\n    total=1\n    font = cv.FONT_HERSHEY_PLAIN  \n    count=0\n    while hasFrames:\n        hasFrames,image = VideoCap.read()\n        if hasFrames:\n            total+=1\n    #        images.append(image)\n            if array[count]!=3:\n                if array[count]==-1:\n                    break\n#                print(array[count])\n                cv.putText(image,'THREAT:'+classes[array[count]],(5, 15),font,1,(0,0,255),1,cv.LINE_4) \n                video.write(image)\n            else:\n                cv.putText(image,'NON-THREAT',(5, 15),font,1,(255, 255, 255),1,cv.LINE_4) \n                video.write(image)\n        count+=1\n    end=perf_counter()\n    print('Time Taken:',end-start,'Video Duration:',total//30)\n    video.release()","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(files_for_demo)):\n    Test = VideoLoader(files_for_demo[i],n_frame=5,sequence_length=SEQUENCE_LENGTH,transform = Transform)\n    TestLoader = torch.utils.data.DataLoader(Test,batch_size=1,num_workers=4,shuffle=False)\n    hierarchical_model(files_for_demo[i],identification_model,classification_nonweighted_model,'non-weighted')\n    hierarchical_model(files_for_demo[i],identification_model,classification_weighted_model,'weighted')\n    single_model(files_for_demo[i],four_class_nonweighted_model,'non-weighted')\n    single_model(files_for_demo[i],four_class_weighted_model,'weighted')\n    ","execution_count":41,"outputs":[{"output_type":"stream","text":"4781\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-3aeb5ba104cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mTest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_for_demo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_frame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEQUENCE_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mTestLoader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mhierarchical_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_for_demo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midentification_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassification_nonweighted_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'non-weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mhierarchical_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_for_demo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midentification_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassification_weighted_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#    single_model(files_for_demo[i],four_class_nonweighted_model,'non-weighted')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-39-89ca6d351108>\u001b[0m in \u001b[0;36mhierarchical_model\u001b[0;34m(FILEPATH, identification_model, classification_model, model_name)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mImages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mResults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentification_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSOFTMAX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"files=os.listdir('./')\n#for file in files:\n    ","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}